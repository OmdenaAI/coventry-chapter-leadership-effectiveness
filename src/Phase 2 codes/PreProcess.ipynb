{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/adel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/adel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/adel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from langdetect import detect\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Sure Text is English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang == 'en'\n",
    "    except:\n",
    "        return False\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Noise   \n",
    "\n",
    "The first step comes with removing the noises in the data; here in the text domain, noise is referred to as something which not related to textual human language, and those come with various nature like special characters, use of parentheses, use of square brackets, white spaces, URL’s and punctuations.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove HTML tag\n",
    "def html_remover(data):\n",
    "  beauti = BeautifulSoup(data,'html.parser')\n",
    "  # print('html_remover out type :', type(beauti.get_text()))\n",
    "  return beauti.get_text()\n",
    "\n",
    "# to remove URL\n",
    "def url_remover(data):\n",
    "  # print('html_remover out type :', type(re.sub(r'https\\S','',data)))\n",
    "  return re.sub(r'https\\S','',data)\n",
    "\n",
    "def web_associated(data):\n",
    "  text = html_remover(data)\n",
    "  text = url_remover(text)\n",
    "  return text\n",
    "\n",
    "# new_data = web_associated(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the HTML tag and URL, there is still some noise in the form of punctuations and white spaces, and text data under the parenthesis; this need to be also treated;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_round_brackets(data):\n",
    "  # print('remove_round_brackets :', type(re.sub('\\(.*?\\)','',data)))\n",
    "  return re.sub('\\(.*?\\)','',data)\n",
    "\n",
    "def remove_punc(data):\n",
    "  trans = str.maketrans('','', string.punctuation)\n",
    "  # print('remove_punc:', type(data.translate(trans)))\n",
    "  return data.translate(trans)\n",
    "\n",
    "def white_space(data):\n",
    "  # print('white_space:', type(' '.join(data.split())))\n",
    "  return ' '.join(data.split())\n",
    "\n",
    "def complete_noise(data):\n",
    "  new_data = remove_round_brackets(data)\n",
    "  new_data = remove_punc(new_data)\n",
    "  new_data = white_space(new_data)\n",
    "  return new_data\n",
    "\n",
    "# new_data = complete_noise(new_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    return emoji.demojize(text, delimiters=(' ', ' '))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing text  \n",
    "\n",
    "Usually, text normalisation starts with tokenizing the text, which our longer corpus is now to be split into chunks of words, which the tokenizer class from NLTK can do. Post that, we need to lower case each word of our corpus, converting numbers to the words and last with contraction replacement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lower(data):\n",
    "  # print('text_lower:', type(data.lower()))\n",
    "  return data.lower()\n",
    "\n",
    "def contraction_replace(data):\n",
    "  # print('contraction_replace:', type(contractions.fix(data)))\n",
    "  return contractions.fix(data)\n",
    "\n",
    "def number_to_text(data):\n",
    "  temp_str = data.split()\n",
    "  string = []\n",
    "  for i in temp_str:\n",
    "    # if the word is digit, converted to \n",
    "    # word else the sequence continues\n",
    "    if i.isdigit():\n",
    "      temp = inflect.engine().number_to_words(i)\n",
    "      string.append(temp)\n",
    "    else:\n",
    "      string.append(i)\n",
    "  # print('number_to_text:', type(temp_str))\n",
    "  return temp_str\n",
    "\n",
    "def normalization(data):\n",
    "  text = text_lower(data)\n",
    "  text = number_to_text(text)\n",
    "  text = contraction_replace(\" \".join(text))\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  # print('normalization:', type(tokens))\n",
    "  return tokens\n",
    "\n",
    "# tokens = normalization(new_data)\n",
    "# print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words, Stemming or lemmatization\n",
    "\n",
    "stopwords have no meaning at all; it is just used for decorative purposes. Therefore, further to reduce dimensionality, it is necessary to remove stopwords from the corpus.  \n",
    "\n",
    "In the end, we have two choices to represent our corpus in the form of stemming or lemmatized words. Stemming usually tries to convert the word into its root format, and mostly it is being carried out by simply cutting words. Where lemmatization also does the task as stemming but in the proper way means it converts the word into roots format like ‘scenes’ will be converted to ‘scene’. One can choose between stemming and lemmatized words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword(data):\n",
    "  clean = []\n",
    "  for i in data:\n",
    "    if i not in stopwords.words('english'):\n",
    "      clean.append(i)\n",
    "  # print('stopword:', type(clean))\n",
    "  return clean\n",
    "\n",
    "def stemming(data):\n",
    "  stemmer = LancasterStemmer()\n",
    "  stemmed = []\n",
    "  for i in data:\n",
    "    stem = stemmer.stem(i)\n",
    "    stemmed.append(stem)\n",
    "  # print('stemming:', type(stemmed))\n",
    "  return stemmed\n",
    "\n",
    "def lemmatization(data):\n",
    "  lemma = WordNetLemmatizer()\n",
    "  lemmas = []\n",
    "  for i in data:\n",
    "    lem = lemma.lemmatize(i, pos='v')\n",
    "    lemmas.append(lem)\n",
    "  # print('lemmatization:', type(lemmas))\n",
    "  return lemmas  \n",
    "\n",
    "def final_process(data):\n",
    "  stopwords_remove = stopword(data)\n",
    "  stemmed = stemming(stopwords_remove)\n",
    "  lemm = lemmatization(stopwords_remove)\n",
    "  return stemmed, lemm\n",
    "# stem,lemmas = final_process(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_pipeline(df):\n",
    "    # print(df)\n",
    "    # print(type(df))\n",
    "    if is_english(df):\n",
    "        new_data = web_associated(df)\n",
    "        new_data = complete_noise(new_data)\n",
    "        new_data = remove_emojis(new_data)\n",
    "        tokens = normalization(new_data)\n",
    "        stem,lemmas = final_process(tokens)\n",
    "\n",
    "        return lemmas\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data frame\n",
    "data = pd.read_csv(\"Africa_task1.1_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_'] = data['tweet'].apply(pre_process_pipeline)\n",
    "data['most_popular_reply_'] = data['most_popular_reply'].apply(pre_process_pipeline)\n",
    "data['second_most_popular_reply_'] = data['second_most_popular_reply'].apply(pre_process_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['username', 'screen_name', 'tweet', 'reply_count', 'like_count',\n",
       "       'quote_count', 'retweet_count', 'created_at_date', 'most_popular_reply',\n",
       "       'most_popular_reply_likes', 'second_most_popular_reply',\n",
       "       'second_most_popular_reply_likes', 'gender', 'country', 'tweet_',\n",
       "       'most_popular_reply_', 'second_most_popular_reply_'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19181/2992850665.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  h_data.drop(['tweet','most_popular_reply', 'second_most_popular_reply'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data.drop(['tweet','most_popular_reply', 'second_most_popular_reply'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19181/3016291159.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  h_data.rename(columns={'tweet_':'tweet', 'most_popular_reply_':'most_popular_reply', 'second_most_popular_reply_':'second_most_popular_reply'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data.rename(columns={'tweet_':'tweet', 'most_popular_reply_':'most_popular_reply', 'second_most_popular_reply_':'second_most_popular_reply'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['username', 'screen_name', 'tweet', 'reply_count', 'like_count',\n",
    "       'quote_count', 'retweet_count', 'created_at_date', 'most_popular_reply',\n",
    "       'most_popular_reply_likes', 'second_most_popular_reply',\n",
    "       'second_most_popular_reply_likes', 'gender', 'country']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Omdena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
