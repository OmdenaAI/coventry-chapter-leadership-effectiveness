{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyyzPixh-rh9"
   },
   "source": [
    "HUGGING FACE SENTIMENT ANALYSIS WITH cardiffnlp Roberta MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15122,
     "status": "ok",
     "timestamp": 1684439114711,
     "user": {
      "displayName": "Trevor W",
      "userId": "10619211731962396369"
     },
     "user_tz": 240
    },
    "id": "A8sEcg_KlyCz",
    "outputId": "4d485e8e-38ca-4f66-f190-86751181b9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "import nltk  # ? necessary\n",
    "!pip install sentencepiece  # ? necessary\n",
    "!pip install emoji\n",
    "import emoji\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16946,
     "status": "ok",
     "timestamp": 1684438424347,
     "user": {
      "displayName": "Trevor W",
      "userId": "10619211731962396369"
     },
     "user_tz": 240
    },
    "id": "H_w0gztRmJLz",
    "outputId": "49a71b65-8d00-41d9-8ebc-795f39dfb6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16254,
     "status": "ok",
     "timestamp": 1684438445935,
     "user": {
      "displayName": "Trevor W",
      "userId": "10619211731962396369"
     },
     "user_tz": 240
    },
    "id": "cMTrOvj73BXa",
    "outputId": "d7ca930a-cca4-4d90-a100-b65ae5d3f6f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# for communicating with associated Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1684439620061,
     "user": {
      "displayName": "Trevor W",
      "userId": "10619211731962396369"
     },
     "user_tz": 240
    },
    "id": "xw3HXZ31KT_4"
   },
   "outputs": [],
   "source": [
    "# to create \"cleaned\" dataset for sentiment analysis\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    if isinstance(tweet, str):\n",
    "\n",
    "        # Remove hashtags\n",
    "        tweet = re.sub(r'#\\w+', '', tweet)\n",
    "        \n",
    "        # Remove URLs\n",
    "        tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "\n",
    "        # Remove HTML tags\n",
    "        clean = re.compile('<.*?>')\n",
    "        tweet = re.sub(clean, '', tweet)\n",
    "\n",
    "        # Remove HTML ref:\n",
    "        clean = re.compile(r\"&[a-z]+;\")\n",
    "        tweet = re.sub(clean,'',tweet)\n",
    "        \n",
    "        # Remove emoticons\n",
    "        emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', tweet)\n",
    "        tweet = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', tweet)\n",
    "\n",
    "        # Remove emojis\n",
    "        tweet = emoji.replace_emoji(tweet,'')\n",
    "        \n",
    "        # Remove additional whitespace\n",
    "        tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "        \n",
    "        return tweet\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/My Drive/NorthAmerica_preprocess_full_1.csv')\n",
    "\n",
    "df = df.head(500)  # DEBUG \n",
    "\n",
    "# Apply the cleaning function to the tweet column\n",
    "df['cleaned_tweet'] = df['text'].apply(clean_tweet)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('/content/drive/My Drive/cleaned_dataset_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuHgFLmcT75_"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1684438717264,
     "user": {
      "displayName": "Trevor W",
      "userId": "10619211731962396369"
     },
     "user_tz": 240
    },
    "id": "ziN5XwoPzsQQ"
   },
   "outputs": [],
   "source": [
    "roberta_sentiment_labels = ['Negative', 'Neutral', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91381,
     "status": "ok",
     "timestamp": 1684440168631,
     "user": {
      "displayName": "Trevor W",
      "userId": "10619211731962396369"
     },
     "user_tz": 240
    },
    "id": "0I_yA2FWPOKh",
    "outputId": "056f9288-e1f2-4e56-82b8-844286f0ee3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on index 0\n",
      "on index 50\n",
      "on index 100\n",
      "on index 150\n",
      "on index 200\n",
      "on index 250\n",
      "on index 300\n",
      "on index 350\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# model has following outputs: Labels: 0 -> Negative; \n",
    "  # 1 -> Neutral; 2 -> Positive\n",
    "SENTIMENT_MODEL_PATH = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL_PATH)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(SENTIMENT_MODEL_PATH)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/My Drive/cleaned_dataset_1.csv').head(400) # DEBUG\n",
    "\n",
    "# Iterate over the dataset rows\n",
    "ct = 0\n",
    "for index, row in df.iterrows():\n",
    "    text = row['cleaned_tweet']\n",
    "\n",
    "    if ct % 50 == 0:\n",
    "        print(f\"on index {ct}\")\n",
    "\n",
    "    # Check if the text is a string\n",
    "    if isinstance(text, str):\n",
    "\n",
    "        # Check if the text is empty\n",
    "        if not text:\n",
    "            final_sentiment = \"unknown\"\n",
    "        else:\n",
    "            # Tokenize the text\n",
    "            sentiment_tokenized_text = sentiment_tokenizer(text, padding=True, \n",
    "                        truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "            # Perform sentiment analysis\n",
    "            sentiment_output = sentiment_model(**sentiment_tokenized_text)\n",
    "            sentiment_label = sentiment_output.logits.argmax().item()\n",
    "\n",
    "            try:\n",
    "                final_sentiment = roberta_sentiment_labels[sentiment_label]\n",
    "            except IndexError:\n",
    "                print(\"problem with sentiment index\")\n",
    "    else:\n",
    "        final_sentiment = \"unknown\"\n",
    "\n",
    "    # Add the final sentiment to the 'sentiment' column\n",
    "    df.at[index, 'sentiment'] = final_sentiment\n",
    "\n",
    "    ct += 1\n",
    "\n",
    "# Save the modified dataset with sentiment column\n",
    "df.to_csv('/content/drive/My Drive/modified_dataset_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaFuzQ8iYMw-"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1684440593063,
     "user": {
      "displayName": "Trevor W",
      "userId": "10619211731962396369"
     },
     "user_tz": 240
    },
    "id": "yKCDqcQyYdiH",
    "outputId": "c750dd7f-573f-4d20-9e23-603842ddc615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet/sentiment: twtr users: spend 3hrs/day on product providing immeasurable new biz, connections, capital, friendships fun Also twtr users: $8/mo for extra features: you know, seems excessive- but I'll cont to pay NFLX $15/mo forever never use it psychological rooting of free is STRONG / Positive\n",
      "tweet/sentiment: Greetings on . May Lord Buddha’s continue to inspire d entire world to strive for peace,harmony,unity n love / Positive\n",
      "tweet/sentiment: Kathy Hochul won’t remove Alvin Bragg as Manhattan DA. I’ll fire him as my very first Day ONE action. like your wallet, safety, and freedoms depend upon it. / Negative\n",
      "tweet/sentiment: XMAS POST With strikes disrupting postal services this month, Royal Mail have brought their last posting dates forward. Make sure gifts cards have been sent by the dates below to ensure they arrive on time! More info on Xmas postal services at / Neutral\n",
      "tweet/sentiment: The COVID vaccine should not be a requirement for students to attend school. The voice of parents is strong and resolute. They do not want the vaccine to be mandated. This is a personal choice for families to make. I stand with these parents and support their position. / Neutral\n",
      "tweet/sentiment: If we don't hang together, we will be hanged separately. / Negative\n",
      "tweet/sentiment: The only thing that matters for building your marketing org (if you care about winning and not political bullshit) is creative talent. I can outsource or automate most everything else. This is the final boss and durable advantage over the long term. Still, almost no one gets it. / Positive\n",
      "tweet/sentiment: At Paje in South District where today we have started our programme in Unguja island. Currently we are in a meeting with CUF constituencies' and branches' leaders in the district. / Neutral\n",
      "tweet/sentiment: The cowardly arrest of @ShireenMazari1 is condemnable and shows the meltdown of this imported regime / Negative\n",
      "tweet/sentiment: Happy Birthday to calm and dynamic @prasanto . Have a great year ahead. / Positive\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_sam = 10\n",
    "tst_lst = random.sample(range(df.shape[0]),num_sam)\n",
    "df_tst = df.iloc[tst_lst]\n",
    "for ii in range(num_sam):\n",
    "  print(f\"tweet/sentiment: {df_tst['cleaned_tweet'].iloc[ii]} / {df_tst['sentiment'].iloc[ii]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laktAOFceZBl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1my3b4hW54fSGIA3R4m3vP1zPK0O7NF13",
     "timestamp": 1684437458000
    },
    {
     "file_id": "1IZAYwlaWQfNOad07jKoku1TKVRreOo2P",
     "timestamp": 1684430074872
    },
    {
     "file_id": "146vZX6x2dRScj03sIaJvdGpNiv1CdvkI",
     "timestamp": 1684426487845
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
